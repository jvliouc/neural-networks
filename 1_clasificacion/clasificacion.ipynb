{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f79131",
   "metadata": {},
   "source": [
    "# 1. Clasificación\n",
    "## 1.1 elementos básicos\n",
    "### Neurona: unidad fundamental de procesamiento de información \n",
    "### Neural model: \n",
    "#### 1. Synapses o conecciones, cada uno caracterizado con un peso $w_{kj}$, donde k corresponde a la neurona y j al input $x_j$ (considerar un vector de m filas).\n",
    "#### 2. Sumatoria: es la combinación lineal de todos los input ponderado con su respectivo peso.\n",
    "#### 3. Función de activación (Squashing function): limita la amplitud a un rango finito.\n",
    "#### 4. External bias $b_k$: NO ES SESGO estadístico, este afecta antes de la función de activación. Hace que la función no pase por el origen\n",
    "$$\n",
    "u_k=\\sum_{j=1}^{m}{x_jw_{kj}}\n",
    "$$\n",
    "$$\n",
    "y_k=\\phi(u_k+b_k)\n",
    "$$\n",
    "#### El external bias, se puede reemplazar por un input $x_0$=+1 y calcular su peso $b_k$\n",
    "<p align=\"center\">\n",
    "    <img src=\"../1_clasificacion/neural1.png\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "### Disclaimer: La relación entre input y output no tiene porqué ser lineal, un input puede ir a varias neuronas, puede haber feedback, etc. en su momento estudiaré eso de ser necesario.\n",
    "## 2.Regresión lineal\n",
    "#### Variable de interes: respuesta o variable dependiente.\n",
    "#### Regresores: variables independientes cuyo rol es explicar o predecir el comportamiento estadístico de la respuesta.\n",
    "#### Expectational error ($\\epsilon$): para agregar las incertezas en la relacion de dependencia de las variables.\n",
    "#### x: vector de inputs (M en total, u orden del modelo), una sola muestra.\n",
    "#### d: respuesta, asumiremos escalar (no vectorial por simplicidad), una sola muestra.\n",
    "#### w: parametros ajustados-> entorno estocastico estacionario desconocido.\n",
    "#### la función:\n",
    "$$\n",
    "d=\\sum_{j=1}^{M}{w_jx_j}+\\epsilon\n",
    "$$\n",
    "#### vectorialmente:\n",
    "$$\n",
    "d=w^Tx+\\epsilon\n",
    "$$\n",
    "#### Extendiendo el modelo a estadisticas conjuntas:\n",
    "#### X: tiene una matriz de correlación (matriz de correlación del regresor)\n",
    "#### D: tiene una varianza de la respuesta deseada.\n",
    "#### X y D tiene una correlación cruzada.\n",
    "#### se asume que X y D tienen media 0. Matricialmente:\n",
    "$$\n",
    "D=w^TX + \\Epsilon\n",
    "$$\n",
    "### 2.1 Máximo a posteriori (bayesiano)\n",
    "#### Paradigma bayesiano (concepto interesante): Entender la incertidumbre como una probabilidad y actualiza el conocimiento cuando entra nueva información -> nada es verdadero o falso, todo tiene un grado de creencia.\n",
    "#### El regresor X actua como la excitacion, sin relacion con el parámetro w\n",
    "#### LA información sobre el parametro W, esta contenida en la respuesta deseada D, lo observable en el entorno\n",
    "#### La función de densidad conjunta W,D condicional a X se puede escribir de dos formas :\n",
    "$$\n",
    "p_{W,D|X}(w,d|x)=p_{W|D,X}(w|d,x)p_D(d)\n",
    "$$\n",
    "$$\n",
    "p_{W,D|X}(w,d|x)=p_{D|W,X}(d|w,x)p_W(w)\n",
    "$$\n",
    "#### Manipulando ambas se obtiene el teorema de bayes:\n",
    "$$\n",
    "p_{W|D,X}(w|d,x)=\\frac{p_{D|W,X}(d|w,x)}{p_D(d)}\n",
    "$$\n",
    "#### 1. Densidad de observación\n",
    "\n",
    "#### Corresponde a la función de densidad de probabilidad condicional  \n",
    "\n",
    "$$\n",
    "p_{D \\mid W, X}(d \\mid w, x)\n",
    "$$\n",
    "\n",
    "#### que se refiere a la “observación” de la respuesta del entorno $d$ debido al regresor $x$, dado el vector de parámetros $w$.\n",
    "\n",
    "\n",
    "#### 2. Prior (Distribución previa)\n",
    "\n",
    "#### Corresponde a la función de densidad de probabilidad  \n",
    "\n",
    "$$\n",
    "p_W(w)\n",
    "$$\n",
    "\n",
    "#### que se refiere a la información sobre el vector de parámetros $w$, antes de realizar cualquier observación del entorno.  \n",
    "\n",
    "#### En adelante, la distribución previa se denota simplemente por  \n",
    "\n",
    "$$\n",
    "\\pi(w)\n",
    "$$\n",
    "\n",
    "\n",
    "#### 3. Densidad posterior\n",
    "\n",
    "#### Corresponde a la función de densidad de probabilidad condicional  \n",
    "\n",
    "$$\n",
    "p_{W \\mid D, X}(w \\mid d, x)\n",
    "$$\n",
    "\n",
    "#### que se refiere al vector de parámetros $w$ “después” de que la observación del entorno ha sido completada.  \n",
    "\n",
    "#### En adelante, la densidad posterior se denota por  \n",
    "\n",
    "$$\n",
    "\\pi(w \\mid d, x)\n",
    "$$\n",
    "\n",
    "#### El par respuesta–regresor $(x, d)$ constituye el “modelo de observación”, el cual incorpora la respuesta $d$ del entorno debido al regresor $x$.\n",
    "\n",
    "\n",
    "#### 4. Evidencia\n",
    "\n",
    "#### Corresponde a la función de densidad de probabilidad  \n",
    "\n",
    "$$\n",
    "p_D(d)\n",
    "$$\n",
    "\n",
    "#### que se refiere a la “información” contenida en la respuesta $d$ para el análisis estadístico.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fb3c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40326188",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
